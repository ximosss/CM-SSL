{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19810654",
   "metadata": {},
   "source": [
    "### PPG_DALIA_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Directory where the input data is located\n",
    "data_dir = \n",
    "# Output directory for the processed .npy files\n",
    "output_dir = \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File matching pattern to find subject data\n",
    "file_pattern = os.path.join(data_dir, 'S*', 'S*.pkl')\n",
    "\n",
    "# Dataset Specifications\n",
    "FS_PPG = 64          # Sampling frequency of the PPG signal\n",
    "WINDOW_SECONDS = 8   # Duration of each data window in seconds\n",
    "SHIFT_SECONDS = 2    # Overlap between consecutive windows in seconds\n",
    "\n",
    "# Split ratios and random seed for reproducibility\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- 2. Find all data files ---\n",
    "file_paths = sorted(glob.glob(file_pattern))\n",
    "\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"Error: No matching files found in '{data_dir}'. Pattern: '{file_pattern}'\")\n",
    "\n",
    "print(f\"Found {len(file_paths)} subject files to process:\")\n",
    "for path in file_paths:\n",
    "    print(f\"- {path}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 3. Process data and group by subject ID ---\n",
    "data_by_subject = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract subject ID from the file path (e.g., 'S5' from '/path/to/S5/S5.pkl')\n",
    "    subject_id = os.path.basename(os.path.dirname(file_path))\n",
    "    print(f\"Processing subject: {subject_id}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "        # Extract PPG signal and corresponding heart rate labels\n",
    "        ppg_signal = data['signal']['wrist']['BVP'].ravel()\n",
    "        labels = data['label'].ravel()\n",
    "\n",
    "        # Calculate window and shift lengths in samples\n",
    "        win_len_samples = int(WINDOW_SECONDS * FS_PPG)\n",
    "        win_shift_samples = int(SHIFT_SECONDS * FS_PPG)\n",
    "        \n",
    "        current_subject_features = []\n",
    "        current_subject_labels = []\n",
    "\n",
    "        # Create sliding windows\n",
    "        num_windows = len(labels)\n",
    "        for i in range(num_windows):\n",
    "            start_idx = i * win_shift_samples\n",
    "            end_idx = start_idx + win_len_samples\n",
    "            \n",
    "            # Ensure the window does not exceed the signal bounds\n",
    "            if end_idx > len(ppg_signal):\n",
    "                break\n",
    "\n",
    "            feature_segment = ppg_signal[start_idx:end_idx]\n",
    "            label_value = labels[i]\n",
    "            \n",
    "            current_subject_features.append(feature_segment)\n",
    "            current_subject_labels.append(label_value)\n",
    "        \n",
    "        # If data was successfully extracted, store it in the dictionary\n",
    "        if current_subject_features:\n",
    "            data_by_subject[subject_id] = {\n",
    "                'features': np.array(current_subject_features),\n",
    "                'labels': np.array(current_subject_labels)\n",
    "            }\n",
    "            print(f\"  -> Successfully extracted {len(current_subject_features)} samples.\")\n",
    "        else:\n",
    "            print(f\"  -> Failed to extract any samples from {subject_id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error processing file '{file_path}': {e}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"All files preprocessed. Data is now grouped by subject ID.\")\n",
    "\n",
    "# --- 4. Perform subject-based data splitting ---\n",
    "\n",
    "# Get all unique subject IDs and shuffle them for random splitting\n",
    "subject_ids = list(data_by_subject.keys())\n",
    "np.random.shuffle(subject_ids)\n",
    "num_subjects = len(subject_ids)\n",
    "\n",
    "if num_subjects < 3:\n",
    "    raise ValueError(\"Number of subjects is less than 3, cannot split into train, validation, and test sets.\")\n",
    "\n",
    "# Split subject IDs based on the defined ratios\n",
    "train_split_idx = int(num_subjects * TRAIN_RATIO)\n",
    "val_split_idx = train_split_idx + int(num_subjects * VAL_RATIO)\n",
    "\n",
    "train_subject_ids = subject_ids[:train_split_idx]\n",
    "val_subject_ids = subject_ids[train_split_idx:val_split_idx]\n",
    "test_subject_ids = subject_ids[val_split_idx:]\n",
    "\n",
    "print(\"Subject ID Split Results:\")\n",
    "print(f\"Training set subjects ({len(train_subject_ids)}): {train_subject_ids}\")\n",
    "print(f\"Validation set subjects ({len(val_subject_ids)}): {val_subject_ids}\")\n",
    "print(f\"Test set subjects ({len(test_subject_ids)}): {test_subject_ids}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 5. Combine data into final sets based on the split IDs ---\n",
    "\n",
    "def combine_data_from_ids(ids_list, data_source):\n",
    "    \"\"\"Helper function to combine data from the source dictionary based on a list of IDs.\"\"\"\n",
    "    if not ids_list:\n",
    "        # If the ID list is empty, return empty arrays with the correct shape.\n",
    "        # Get dimension info from a sample entry to create correctly shaped empty arrays.\n",
    "        if not data_source:\n",
    "            return np.empty((0, WINDOW_SECONDS * FS_PPG)), np.empty((0,))\n",
    "        \n",
    "        sample_key = next(iter(data_source))\n",
    "        feature_dim = data_source[sample_key]['features'].shape[1]\n",
    "        return np.empty((0, feature_dim)), np.empty((0,))\n",
    "        \n",
    "    features_list = [data_source[pid]['features'] for pid in ids_list]\n",
    "    labels_list = [data_source[pid]['labels'] for pid in ids_list]\n",
    "    \n",
    "    # Use np.concatenate to merge the data from all subjects in the list\n",
    "    final_features = np.concatenate(features_list, axis=0)\n",
    "    final_labels = np.concatenate(labels_list, axis=0)\n",
    "    \n",
    "    return final_features, final_labels\n",
    "\n",
    "# Create the final training, validation, and test sets\n",
    "train_features, train_labels = combine_data_from_ids(train_subject_ids, data_by_subject)\n",
    "val_features, val_labels = combine_data_from_ids(val_subject_ids, data_by_subject)\n",
    "test_features, test_labels = combine_data_from_ids(test_subject_ids, data_by_subject)\n",
    "\n",
    "# --- 6. Print final shapes and perform validation ---\n",
    "print(\"Final Dataset Shapes (by sample count):\")\n",
    "print(f\"Training set (Train)  -> Features: {train_features.shape}, Labels: {train_labels.shape}\")\n",
    "print(f\"Validation set (Val)  -> Features: {val_features.shape}, Labels: {val_labels.shape}\")\n",
    "print(f\"Test set (Test)       -> Features: {test_features.shape}, Labels: {test_labels.shape}\")\n",
    "\n",
    "# Verify that the total number of samples matches before and after the split\n",
    "total_samples_in_dict = sum(len(d['labels']) for d in data_by_subject.values())\n",
    "total_samples_after_split = len(train_features) + len(val_features) + len(test_features)\n",
    "assert total_samples_in_dict == total_samples_after_split, \"Total number of samples does not match after splitting!\"\n",
    "print(f\"\\nSample count validation successful: {total_samples_after_split}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 7. Save the processed data to .npy files ---\n",
    "print(f\"Saving files to directory: '{output_dir}'\")\n",
    "\n",
    "np.save(os.path.join(output_dir, 'train_features.npy'), train_features)\n",
    "np.save(os.path.join(output_dir, 'train_labels.npy'), train_labels)\n",
    "\n",
    "np.save(os.path.join(output_dir, 'val_features.npy'), val_features)\n",
    "np.save(os.path.join(output_dir, 'val_labels.npy'), val_labels)\n",
    "\n",
    "np.save(os.path.join(output_dir, 'test_features.npy'), test_features)\n",
    "np.save(os.path.join(output_dir, 'test_labels.npy'), test_labels)\n",
    "\n",
    "print(\"\\nAll files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47e709",
   "metadata": {},
   "source": [
    "### PPG_DALIA_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, welch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Configuration Parameters ---\n",
    "PPG_SAMPLING_RATE = 64\n",
    "RESP_SAMPLING_RATE = 700\n",
    "PPG_SEGMENT_LENGTH = 1250  # Corresponds to ~19.5 seconds\n",
    "SEGMENT_DURATION_S = PPG_SEGMENT_LENGTH / PPG_SAMPLING_RATE\n",
    "RESP_SEGMENT_LENGTH = int(SEGMENT_DURATION_S * RESP_SAMPLING_RATE)\n",
    "STEP_DURATION_S = 1.0  # Sliding window step\n",
    "PPG_STEP_SIZE = int(STEP_DURATION_S * PPG_SAMPLING_RATE)\n",
    "RESP_STEP_SIZE = int(STEP_DURATION_S * RESP_SAMPLING_RATE)\n",
    "RESP_RATE_MIN_HZ = 0.1  # 6 breaths per minute\n",
    "RESP_RATE_MAX_HZ = 0.5  # 30 breaths per minute\n",
    "\n",
    "# --- 2. Respiration Rate Estimation Function ---\n",
    "def estimate_respiration_rate(resp_segment, fs):\n",
    "    \"\"\"\n",
    "    Estimate respiration rate from a respiration signal segment using Welch's method.\n",
    "    \"\"\"\n",
    "    lowcut, highcut = 0.08, 2.0\n",
    "    nyquist = 0.5 * fs\n",
    "    low, high = lowcut / nyquist, highcut / nyquist\n",
    "    \n",
    "    try:\n",
    "        b, a = butter(2, [low, high], btype='band')\n",
    "        filtered_segment = filtfilt(b, a, resp_segment)\n",
    "    except ValueError:\n",
    "        # This can happen if the signal is too short or contains NaNs\n",
    "        return np.nan\n",
    "    \n",
    "    freqs, psd = welch(filtered_segment, fs, nperseg=len(filtered_segment), window='hamming')\n",
    "    \n",
    "    # Find frequencies within the valid respiration rate range\n",
    "    valid_indices = np.where((freqs >= RESP_RATE_MIN_HZ) & (freqs <= RESP_RATE_MAX_HZ))\n",
    "    \n",
    "    if len(valid_indices[0]) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Find the frequency with the highest power in the valid range\n",
    "    valid_psd = psd[valid_indices]\n",
    "    peak_index_in_valid = np.argmax(valid_psd)\n",
    "    resp_rate_hz = freqs[valid_indices][peak_index_in_valid]\n",
    "    \n",
    "    return resp_rate_hz\n",
    "\n",
    "# --- 3. Data Processing Function ---\n",
    "def process_subject_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes a single subject's .pkl file.\n",
    "    Returns features (PPG segments) and labels (respiration rates), or (None, None) \n",
    "    if an error occurs or no valid segments are found.\n",
    "    \"\"\"\n",
    "    subject_id = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "        bvp_signal = data['signal']['wrist']['BVP'].flatten()\n",
    "        resp_signal = data['signal']['chest']['Resp'].flatten()\n",
    "    except (FileNotFoundError, KeyError, EOFError):\n",
    "        return None, None\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Determine the maximum number of windows that can be created\n",
    "    max_ppg_start = len(bvp_signal) - PPG_SEGMENT_LENGTH\n",
    "    max_resp_start = len(resp_signal) - RESP_SEGMENT_LENGTH\n",
    "\n",
    "    if max_ppg_start < 0 or max_resp_start < 0:\n",
    "        return None, None\n",
    "\n",
    "    total_iterations = min(max_ppg_start // PPG_STEP_SIZE, max_resp_start // RESP_STEP_SIZE) + 1\n",
    "    \n",
    "    for i in tqdm(range(total_iterations), desc=f\"Processing {subject_id}\", leave=False, ncols=100):\n",
    "        ppg_start_idx = i * PPG_STEP_SIZE\n",
    "        resp_start_idx = i * RESP_STEP_SIZE\n",
    "        \n",
    "        ppg_segment = bvp_signal[ppg_start_idx : ppg_start_idx + PPG_SEGMENT_LENGTH]\n",
    "        resp_segment = resp_signal[resp_start_idx : resp_start_idx + RESP_SEGMENT_LENGTH]\n",
    "        \n",
    "        estimated_rr = estimate_respiration_rate(resp_segment, RESP_SAMPLING_RATE)\n",
    "        \n",
    "        if not np.isnan(estimated_rr):\n",
    "            features_list.append(ppg_segment)\n",
    "            labels_list.append(estimated_rr)\n",
    "\n",
    "    if not features_list:\n",
    "        return None, None\n",
    "        \n",
    "    return np.array(features_list), np.array(labels_list)\n",
    "\n",
    "# --- 4. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    BASE_DIR = \n",
    "    OUTPUT_DIR = \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    NUM_SUBJECTS = 15\n",
    "    TRAIN_RATIO = 0.8\n",
    "    VAL_RATIO = 0.1\n",
    "    RANDOM_SEED = 42\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    # --- Step 1: Process data and store it grouped by subject ---\n",
    "    data_by_subject = {}\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Starting batch processing for {NUM_SUBJECTS} subjects...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i in range(1, NUM_SUBJECTS + 1):\n",
    "        subject_id = f'S{i}'\n",
    "        file_path = os.path.join(BASE_DIR, subject_id, f'{subject_id}.pkl')\n",
    "        \n",
    "        subject_features, subject_labels = process_subject_file(file_path)\n",
    "        \n",
    "        if subject_features is not None and subject_labels is not None:\n",
    "            data_by_subject[subject_id] = {\n",
    "                'features': subject_features,\n",
    "                'labels': subject_labels\n",
    "            }\n",
    "            print(f\"-> Collected {len(subject_features)} samples from {subject_id}.\")\n",
    "        else:\n",
    "            print(f\"-> No valid data collected from {subject_id}.\")\n",
    "\n",
    "    # --- Step 2: Split data based on subject IDs ---\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"All subjects processed. Splitting data by subject ID...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    subject_ids = list(data_by_subject.keys())\n",
    "    np.random.shuffle(subject_ids)\n",
    "    num_subjects_found = len(subject_ids)\n",
    "\n",
    "    if num_subjects_found < 3:\n",
    "        raise ValueError(f\"Found only {num_subjects_found} subjects with valid data. Cannot split into train/val/test sets.\")\n",
    "\n",
    "    train_split_idx = int(num_subjects_found * TRAIN_RATIO)\n",
    "    val_split_idx = train_split_idx + int(num_subjects_found * VAL_RATIO)\n",
    "\n",
    "    train_subject_ids = subject_ids[:train_split_idx]\n",
    "    val_subject_ids = subject_ids[train_split_idx:val_split_idx]\n",
    "    test_subject_ids = subject_ids[val_split_idx:]\n",
    "\n",
    "    print(\"Subject ID split results:\")\n",
    "    print(f\"Train subjects ({len(train_subject_ids)}): {train_subject_ids}\")\n",
    "    print(f\"Validation subjects ({len(val_subject_ids)}): {val_subject_ids}\")\n",
    "    print(f\"Test subjects ({len(test_subject_ids)}): {test_subject_ids}\")\n",
    "\n",
    "    # --- Step 3: Combine data into sets based on the split IDs ---\n",
    "    def combine_data_from_ids(ids_list, data_source):\n",
    "        \"\"\"Helper function to combine data from the source dictionary based on a list of IDs.\"\"\"\n",
    "        if not ids_list:\n",
    "            # If the ID list is empty, return correctly shaped empty arrays.\n",
    "            if not data_source:\n",
    "                return np.empty((0, PPG_SEGMENT_LENGTH)), np.empty((0,))\n",
    "            sample_key = next(iter(data_source))\n",
    "            feature_dim = data_source[sample_key]['features'].shape[1]\n",
    "            return np.empty((0, feature_dim)), np.empty((0,))\n",
    "\n",
    "        features_list = [data_source[pid]['features'] for pid in ids_list]\n",
    "        labels_list = [data_source[pid]['labels'] for pid in ids_list]\n",
    "        return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    train_features, train_labels = combine_data_from_ids(train_subject_ids, data_by_subject)\n",
    "    val_features, val_labels = combine_data_from_ids(val_subject_ids, data_by_subject)\n",
    "    test_features, test_labels = combine_data_from_ids(test_subject_ids, data_by_subject)\n",
    "\n",
    "    # --- Step 4: Validate and save the final datasets ---\n",
    "    print(\"\\n--- Final Aggregated Datasets ---\")\n",
    "    print(f\"Train Set      -> Features: {train_features.shape}, Labels: {train_labels.shape}\")\n",
    "    print(f\"Validation Set -> Features: {val_features.shape}, Labels: {val_labels.shape}\")\n",
    "    print(f\"Test Set       -> Features: {test_features.shape}, Labels: {test_labels.shape}\")\n",
    "\n",
    "    # Verify total sample count\n",
    "    total_samples = sum(len(d['labels']) for d in data_by_subject.values())\n",
    "    split_samples = len(train_labels) + len(val_labels) + len(test_labels)\n",
    "    assert total_samples == split_samples, \"Sample count mismatch after split!\"\n",
    "    print(f\"\\nTotal sample count verified: {total_samples}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Saving final files to '{OUTPUT_DIR}'...\")\n",
    "    \n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_features.npy'), train_features)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_labels.npy'), train_labels)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_features.npy'), val_features)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_labels.npy'), val_labels)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_features.npy'), test_features)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_labels.npy'), test_labels)\n",
    "\n",
    "    print(\"All files saved successfully.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9ccf8",
   "metadata": {},
   "source": [
    "### BIDMC_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f783926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "DATASET_PATH = \n",
    "OUTPUT_PATH = \n",
    "NUM_SUBJECTS = 53\n",
    "SAMPLING_RATE = 125\n",
    "ANNOTATOR_COLUMN = 'breaths ann1 [signal sample no]'\n",
    "\n",
    "# Windowing parameters\n",
    "WINDOW_SEC = 10\n",
    "STRIDE_SEC = 2\n",
    "\n",
    "# Splitting parameters\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "RANDOM_SEED = 42 # For reproducible splits\n",
    "\n",
    "# --- 2. Single Subject Processing Function ---\n",
    "def create_windows_and_labels_for_subject(subject_id):\n",
    "    \"\"\"\n",
    "    Creates PPG windows and corresponding respiration rate labels for a single subject.\n",
    "    \"\"\"\n",
    "    signals_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Signals.csv')\n",
    "    breaths_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Breaths.csv')\n",
    "\n",
    "    if not os.path.exists(signals_file) or not os.path.exists(breaths_file):\n",
    "        print(f\"Warning: Files for subject {subject_id} are missing. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        signals_df = pd.read_csv(signals_file)\n",
    "        signals_df.columns = signals_df.columns.str.strip()\n",
    "        \n",
    "        breaths_df = pd.read_csv(breaths_file)\n",
    "        breaths_df.columns = breaths_df.columns.str.strip()\n",
    "\n",
    "        ppg_signal = signals_df['PLETH'].values\n",
    "        breath_starts = breaths_df[ANNOTATOR_COLUMN].values\n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Error: Could not read or process files for subject {subject_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    window_samples = WINDOW_SEC * SAMPLING_RATE\n",
    "    stride_samples = STRIDE_SEC * SAMPLING_RATE\n",
    "\n",
    "    ppg_windows = []\n",
    "    rr_labels = []\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx + window_samples <= len(ppg_signal):\n",
    "        end_idx = start_idx + window_samples\n",
    "        current_ppg_window = ppg_signal[start_idx:end_idx]\n",
    "        \n",
    "        # Count breaths within the current window\n",
    "        breaths_in_window = np.sum((breath_starts >= start_idx) & (breath_starts < end_idx))\n",
    "        # Convert count to breaths per minute\n",
    "        respiratory_rate = (breaths_in_window / WINDOW_SEC) * 60\n",
    "        \n",
    "        ppg_windows.append(current_ppg_window)\n",
    "        rr_labels.append(respiratory_rate)\n",
    "        \n",
    "        start_idx += stride_samples\n",
    "\n",
    "    if not ppg_windows:\n",
    "        return None, None\n",
    "        \n",
    "    return np.array(ppg_windows), np.array(rr_labels)\n",
    "\n",
    "\n",
    "# --- 3. Main Execution Function ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process all subject data, split by subject ID, and save the results.\n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # --- Step 1: Process and group data by subject ---\n",
    "    data_by_subject = {}\n",
    "    print(\"Starting dataset processing...\")\n",
    "\n",
    "    for i in range(1, NUM_SUBJECTS + 1):\n",
    "        subject_id = i\n",
    "        features, labels = create_windows_and_labels_for_subject(subject_id)\n",
    "        \n",
    "        if features is not None and labels is not None:\n",
    "            data_by_subject[subject_id] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            print(f\"Processed subject {subject_id:02} - Generated {len(features)} window samples.\")\n",
    "\n",
    "    if not data_by_subject:\n",
    "        print(\"Error: Failed to generate any samples from the dataset. Please check paths and files.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Split subjects into train, validation, and test sets ---\n",
    "    print(\"\\nAll subjects processed. Splitting dataset by subject ID...\")\n",
    "    \n",
    "    subject_ids = list(data_by_subject.keys())\n",
    "    np.random.shuffle(subject_ids)\n",
    "    num_subjects_found = len(subject_ids)\n",
    "\n",
    "    if num_subjects_found < 3:\n",
    "        raise ValueError(f\"Found only {num_subjects_found} subjects with valid data. Cannot split into train/val/test sets.\")\n",
    "\n",
    "    train_split_idx = int(num_subjects_found * TRAIN_RATIO)\n",
    "    val_split_idx = train_split_idx + int(num_subjects_found * VAL_RATIO)\n",
    "\n",
    "    train_subject_ids = subject_ids[:train_split_idx]\n",
    "    val_subject_ids = subject_ids[train_split_idx:val_split_idx]\n",
    "    test_subject_ids = subject_ids[val_split_idx:]\n",
    "    \n",
    "    print(\"\\nSubject ID Split Results:\")\n",
    "    print(f\"Training set subjects ({len(train_subject_ids)}): {sorted(train_subject_ids)}\")\n",
    "    print(f\"Validation set subjects ({len(val_subject_ids)}): {sorted(val_subject_ids)}\")\n",
    "    print(f\"Test set subjects ({len(test_subject_ids)}): {sorted(test_subject_ids)}\")\n",
    "\n",
    "    # --- Step 3: Combine data based on the split subject IDs ---\n",
    "    def combine_data_from_ids(ids_list, data_source):\n",
    "        \"\"\"Helper function to combine data arrays from a list of subject IDs.\"\"\"\n",
    "        window_samples = WINDOW_SEC * SAMPLING_RATE\n",
    "        if not ids_list:\n",
    "            return np.empty((0, window_samples)), np.empty((0,))\n",
    "        \n",
    "        features_list = [data_source[sid]['features'] for sid in ids_list]\n",
    "        labels_list = [data_source[sid]['labels'] for sid in ids_list]\n",
    "        return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    train_features, train_labels = combine_data_from_ids(train_subject_ids, data_by_subject)\n",
    "    val_features, val_labels = combine_data_from_ids(val_subject_ids, data_by_subject)\n",
    "    test_features, test_labels = combine_data_from_ids(test_subject_ids, data_by_subject)\n",
    "\n",
    "    # --- Step 4: Validate and save the final datasets ---\n",
    "    print(\"\\n--- Final Dataset Shapes ---\")\n",
    "    print(f\"Training set (Train)  -> Features: {train_features.shape}, Labels: {train_labels.shape}\")\n",
    "    print(f\"Validation set (Val)  -> Features: {val_features.shape}, Labels: {val_labels.shape}\")\n",
    "    print(f\"Test set (Test)       -> Features: {test_features.shape}, Labels: {test_labels.shape}\")\n",
    "    \n",
    "    total_samples_in_dict = sum(len(d['labels']) for d in data_by_subject.values())\n",
    "    total_samples_after_split = len(train_labels) + len(val_labels) + len(test_labels)\n",
    "    assert total_samples_in_dict == total_samples_after_split, \"Total number of samples does not match after splitting!\"\n",
    "    print(f\"\\nSample count validation successful: {total_samples_after_split}\")\n",
    "\n",
    "    print(f\"\\nSaving files to: '{OUTPUT_PATH}'\")\n",
    "    \n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_features.npy'), train_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_labels.npy'), train_labels)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_features.npy'), val_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_labels.npy'), val_labels)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_features.npy'), test_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_labels.npy'), test_labels)\n",
    "\n",
    "    print(\"All files saved successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dac0f6",
   "metadata": {},
   "source": [
    "### BIDMC_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. 全局参数定义 (部分修改) ---\n",
    "DATASET_PATH = '/home/ubuntu/wokrspace/Data/physionet.org/files/bidmc/1.0.0/bidmc_csv/'\n",
    "NUM_SUBJECTS = 53\n",
    "SAMPLING_RATE = 125\n",
    "\n",
    "WINDOW_SEC = 10\n",
    "STRIDE_SEC = 2\n",
    "\n",
    "# --- 新增/修改的配置 ---\n",
    "OUTPUT_PATH = '/home/ubuntu/wokrspace/Finetuning_tasks/bidmc_hr' # 新的输出文件夹，按受试者划分\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "RANDOM_SEED = 42 # 确保划分可复现\n",
    "\n",
    "# --- 2. 单个受试者处理函数 (保持不变) ---\n",
    "def create_windows_and_labels_for_subject(subject_id):\n",
    "    \"\"\"\n",
    "    为单个受试者创建PPG窗口和对应的平均心率(HR)标签。\n",
    "    (此函数功能正确，无需修改)\n",
    "    \"\"\"\n",
    "    signals_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Signals.csv')\n",
    "    numerics_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Numerics.csv')\n",
    "\n",
    "    if not os.path.exists(signals_file) or not os.path.exists(numerics_file):\n",
    "        print(f\"警告: 受试者 {subject_id:02} 的文件缺失，已跳过。\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        signals_df = pd.read_csv(signals_file)\n",
    "        signals_df.columns = signals_df.columns.str.strip()\n",
    "        \n",
    "        numerics_df = pd.read_csv(numerics_file)\n",
    "        numerics_df.columns = numerics_df.columns.str.strip()\n",
    "\n",
    "        ppg_signal = signals_df['PLETH'].values\n",
    "        \n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"错误: 读取或处理受试者 {subject_id:02} 的文件时出错: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    window_samples = WINDOW_SEC * SAMPLING_RATE\n",
    "    stride_samples = STRIDE_SEC * SAMPLING_RATE\n",
    "\n",
    "    ppg_windows = []\n",
    "    hr_labels = []\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx + window_samples <= len(ppg_signal):\n",
    "        end_idx = start_idx + window_samples\n",
    "        current_ppg_window = ppg_signal[start_idx:end_idx]\n",
    "        \n",
    "        start_time_sec = start_idx / SAMPLING_RATE\n",
    "        end_time_sec = end_idx / SAMPLING_RATE\n",
    "        \n",
    "        relevant_numerics = numerics_df[\n",
    "            (numerics_df['Time [s]'] >= start_time_sec) & \n",
    "            (numerics_df['Time [s]'] < end_time_sec)\n",
    "        ]\n",
    "        \n",
    "        if not relevant_numerics.empty:\n",
    "            average_hr = relevant_numerics['HR'].mean()\n",
    "            if not np.isnan(average_hr): # 确保平均值有效\n",
    "                ppg_windows.append(current_ppg_window)\n",
    "                hr_labels.append(average_hr)\n",
    "        \n",
    "        start_idx += stride_samples\n",
    "\n",
    "    if not ppg_windows:\n",
    "        return None, None\n",
    "        \n",
    "    return np.array(ppg_windows), np.array(hr_labels)\n",
    "\n",
    "\n",
    "# --- 3. 主执行函数 (*** 这里是主要修改的地方 ***) ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，处理所有受试者数据，按受试者ID划分，并保存结果。\n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # --- 步骤 1: 按受试者处理并分组存储数据 ---\n",
    "    data_by_subject = {}\n",
    "    print(\"开始处理数据集，提取PPG窗口和对应的平均HR...\")\n",
    "\n",
    "    for i in range(1, NUM_SUBJECTS + 1):\n",
    "        subject_id = i\n",
    "        features, labels = create_windows_and_labels_for_subject(subject_id)\n",
    "        \n",
    "        if features is not None and labels is not None:\n",
    "            data_by_subject[subject_id] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            print(f\"处理完成: 受试者 {subject_id:02} - 生成了 {len(features)} 个样本。\")\n",
    "\n",
    "    if not data_by_subject:\n",
    "        print(\"错误: 未能从数据集中生成任何样本。请检查路径和文件。\")\n",
    "        return\n",
    "\n",
    "    # --- 步骤 2: 基于受试者ID进行分组划分 ---\n",
    "    print(\"\\n所有受试者处理完毕，正在按ID划分数据集...\")\n",
    "    \n",
    "    subject_ids = list(data_by_subject.keys())\n",
    "    np.random.shuffle(subject_ids)\n",
    "    num_subjects = len(subject_ids)\n",
    "\n",
    "    if num_subjects < 3:\n",
    "        raise ValueError(\"有效受试者数量少于3，无法进行划分。\")\n",
    "\n",
    "    train_split_idx = int(num_subjects * TRAIN_RATIO)\n",
    "    val_split_idx = train_split_idx + int(num_subjects * VAL_RATIO)\n",
    "\n",
    "    train_subject_ids = subject_ids[:train_split_idx]\n",
    "    val_subject_ids = subject_ids[train_split_idx:val_split_idx]\n",
    "    test_subject_ids = subject_ids[val_split_idx:]\n",
    "    \n",
    "    print(\"\\n受试者ID划分结果:\")\n",
    "    print(f\"训练集受试者 ({len(train_subject_ids)}): {sorted(train_subject_ids)}\")\n",
    "    print(f\"验证集受试者 ({len(val_subject_ids)}): {sorted(val_subject_ids)}\")\n",
    "    print(f\"测试集受试者 ({len(test_subject_ids)}): {sorted(test_subject_ids)}\")\n",
    "\n",
    "    # --- 步骤 3: 根据ID划分合并数据 ---\n",
    "    def combine_data_from_ids(ids_list, data_source):\n",
    "        if not ids_list:\n",
    "            # 获取特征维度以创建正确形状的空数组\n",
    "            sample_key = next(iter(data_source))\n",
    "            feature_dim = data_source[sample_key]['features'].shape[1]\n",
    "            return np.empty((0, feature_dim)), np.empty((0,))\n",
    "            \n",
    "        features_list = [data_source[sid]['features'] for sid in ids_list]\n",
    "        labels_list = [data_source[sid]['labels'] for sid in ids_list]\n",
    "        return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    train_features, train_labels = combine_data_from_ids(train_subject_ids, data_by_subject)\n",
    "    val_features, val_labels = combine_data_from_ids(val_subject_ids, data_by_subject)\n",
    "    test_features, test_labels = combine_data_from_ids(test_subject_ids, data_by_subject)\n",
    "\n",
    "    # --- 步骤 4: 验证并保存 ---\n",
    "    print(\"\\n--- 最终数据集形状 ---\")\n",
    "    print(f\"训练集 (Train)  -> 特征: {train_features.shape}, 标签: {train_labels.shape}\")\n",
    "    print(f\"验证集 (Val)    -> 特征: {val_features.shape}, 标签: {val_labels.shape}\")\n",
    "    print(f\"测试集 (Test)   -> 特征: {test_features.shape}, 标签: {test_labels.shape}\")\n",
    "    \n",
    "    total_samples = sum(len(d['labels']) for d in data_by_subject.values())\n",
    "    split_samples = len(train_labels) + len(val_labels) + len(test_labels)\n",
    "    assert total_samples == split_samples, \"样本总数在划分后不匹配！\"\n",
    "    print(f\"\\n样本总数验证成功: {total_samples}\")\n",
    "\n",
    "    print(f\"\\n正在保存文件至: '{OUTPUT_PATH}'\")\n",
    "    \n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_features.npy'), train_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_labels.npy'), train_labels)\n",
    "\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_features.npy'), val_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_labels.npy'), val_labels)\n",
    "\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_features.npy'), test_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_labels.npy'), test_labels)\n",
    "\n",
    "    print(\"所有文件保存成功！\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938782d",
   "metadata": {},
   "source": [
    "### UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Global Parameters ---\n",
    "DATASET_PATH = \n",
    "OUTPUT_PATH = \n",
    "NUM_SUBJECTS = 53\n",
    "SAMPLING_RATE = 125\n",
    "\n",
    "# Windowing parameters\n",
    "WINDOW_SEC = 10\n",
    "STRIDE_SEC = 2\n",
    "\n",
    "# Splitting parameters\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "RANDOM_SEED = 42 # For reproducible splits\n",
    "\n",
    "# --- 2. Single Subject Processing Function ---\n",
    "def create_windows_and_labels_for_subject(subject_id):\n",
    "    \"\"\"\n",
    "    Creates PPG windows and corresponding mean heart rate (HR) labels for a single subject.\n",
    "    \"\"\"\n",
    "    signals_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Signals.csv')\n",
    "    numerics_file = os.path.join(DATASET_PATH, f'bidmc_{subject_id:02}_Numerics.csv')\n",
    "\n",
    "    if not os.path.exists(signals_file) or not os.path.exists(numerics_file):\n",
    "        print(f\"Warning: Files for subject {subject_id:02} are missing. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        signals_df = pd.read_csv(signals_file)\n",
    "        signals_df.columns = signals_df.columns.str.strip()\n",
    "        \n",
    "        numerics_df = pd.read_csv(numerics_file)\n",
    "        numerics_df.columns = numerics_df.columns.str.strip()\n",
    "\n",
    "        ppg_signal = signals_df['PLETH'].values\n",
    "        \n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Error: Could not read or process files for subject {subject_id:02}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    window_samples = WINDOW_SEC * SAMPLING_RATE\n",
    "    stride_samples = STRIDE_SEC * SAMPLING_RATE\n",
    "\n",
    "    ppg_windows = []\n",
    "    hr_labels = []\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx + window_samples <= len(ppg_signal):\n",
    "        end_idx = start_idx + window_samples\n",
    "        current_ppg_window = ppg_signal[start_idx:end_idx]\n",
    "        \n",
    "        # Determine the time range of the current window\n",
    "        start_time_sec = start_idx / SAMPLING_RATE\n",
    "        end_time_sec = end_idx / SAMPLING_RATE\n",
    "        \n",
    "        # Filter numerics data to find HR values within this window\n",
    "        relevant_numerics = numerics_df[\n",
    "            (numerics_df['Time [s]'] >= start_time_sec) & \n",
    "            (numerics_df['Time [s]'] < end_time_sec)\n",
    "        ]\n",
    "        \n",
    "        if not relevant_numerics.empty:\n",
    "            average_hr = relevant_numerics['HR'].mean()\n",
    "            # Ensure the calculated mean is a valid number\n",
    "            if not np.isnan(average_hr):\n",
    "                ppg_windows.append(current_ppg_window)\n",
    "                hr_labels.append(average_hr)\n",
    "        \n",
    "        start_idx += stride_samples\n",
    "\n",
    "    if not ppg_windows:\n",
    "        return None, None\n",
    "        \n",
    "    return np.array(ppg_windows), np.array(hr_labels)\n",
    "\n",
    "\n",
    "# --- 3. Main Execution Function ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process all subject data, split by subject ID, and save the results.\n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # --- Step 1: Process and group data by subject ---\n",
    "    data_by_subject = {}\n",
    "    print(\"Starting dataset processing to extract PPG windows and corresponding mean HR...\")\n",
    "\n",
    "    for i in range(1, NUM_SUBJECTS + 1):\n",
    "        subject_id = i\n",
    "        features, labels = create_windows_and_labels_for_subject(subject_id)\n",
    "        \n",
    "        if features is not None and labels is not None:\n",
    "            data_by_subject[subject_id] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            print(f\"Processed subject {subject_id:02} - Generated {len(features)} samples.\")\n",
    "\n",
    "    if not data_by_subject:\n",
    "        print(\"Error: Failed to generate any samples from the dataset. Please check paths and files.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Split subjects into train, validation, and test sets ---\n",
    "    print(\"\\nAll subjects processed. Splitting dataset by subject ID...\")\n",
    "    \n",
    "    subject_ids = list(data_by_subject.keys())\n",
    "    np.random.shuffle(subject_ids)\n",
    "    num_subjects_found = len(subject_ids)\n",
    "\n",
    "    if num_subjects_found < 3:\n",
    "        raise ValueError(f\"Found only {num_subjects_found} subjects with valid data. Cannot split into train/val/test sets.\")\n",
    "\n",
    "    train_split_idx = int(num_subjects_found * TRAIN_RATIO)\n",
    "    val_split_idx = train_split_idx + int(num_subjects_found * VAL_RATIO)\n",
    "\n",
    "    train_subject_ids = subject_ids[:train_split_idx]\n",
    "    val_subject_ids = subject_ids[train_split_idx:val_split_idx]\n",
    "    test_subject_ids = subject_ids[val_split_idx:]\n",
    "    \n",
    "    print(\"\\nSubject ID Split Results:\")\n",
    "    print(f\"Training set subjects ({len(train_subject_ids)}): {sorted(train_subject_ids)}\")\n",
    "    print(f\"Validation set subjects ({len(val_subject_ids)}): {sorted(val_subject_ids)}\")\n",
    "    print(f\"Test set subjects ({len(test_subject_ids)}): {sorted(test_subject_ids)}\")\n",
    "\n",
    "    # --- Step 3: Combine data based on the split subject IDs ---\n",
    "    def combine_data_from_ids(ids_list, data_source):\n",
    "        \"\"\"Helper function to combine data arrays from a list of subject IDs.\"\"\"\n",
    "        if not ids_list:\n",
    "            # If the list is empty, get feature dimension from a sample to create correctly shaped empty arrays.\n",
    "            if not data_source:\n",
    "                return np.empty((0, WINDOW_SEC * SAMPLING_RATE)), np.empty((0,))\n",
    "            sample_key = next(iter(data_source))\n",
    "            feature_dim = data_source[sample_key]['features'].shape[1]\n",
    "            return np.empty((0, feature_dim)), np.empty((0,))\n",
    "            \n",
    "        features_list = [data_source[sid]['features'] for sid in ids_list]\n",
    "        labels_list = [data_source[sid]['labels'] for sid in ids_list]\n",
    "        return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    train_features, train_labels = combine_data_from_ids(train_subject_ids, data_by_subject)\n",
    "    val_features, val_labels = combine_data_from_ids(val_subject_ids, data_by_subject)\n",
    "    test_features, test_labels = combine_data_from_ids(test_subject_ids, data_by_subject)\n",
    "\n",
    "    # --- Step 4: Validate and save the final datasets ---\n",
    "    print(\"\\n--- Final Dataset Shapes ---\")\n",
    "    print(f\"Training set (Train)  -> Features: {train_features.shape}, Labels: {train_labels.shape}\")\n",
    "    print(f\"Validation set (Val)  -> Features: {val_features.shape}, Labels: {val_labels.shape}\")\n",
    "    print(f\"Test set (Test)       -> Features: {test_features.shape}, Labels: {test_labels.shape}\")\n",
    "    \n",
    "    total_samples_in_dict = sum(len(d['labels']) for d in data_by_subject.values())\n",
    "    total_samples_after_split = len(train_labels) + len(val_labels) + len(test_labels)\n",
    "    assert total_samples_in_dict == total_samples_after_split, \"Total number of samples does not match after splitting!\"\n",
    "    print(f\"\\nSample count validation successful: {total_samples_after_split}\")\n",
    "\n",
    "    print(f\"\\nSaving files to: '{OUTPUT_PATH}'\")\n",
    "    \n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_features.npy'), train_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'train_labels.npy'), train_labels)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_features.npy'), val_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'val_labels.npy'), val_labels)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_features.npy'), test_features)\n",
    "    np.save(os.path.join(OUTPUT_PATH, 'test_labels.npy'), test_labels)\n",
    "\n",
    "    print(\"All files saved successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824f091",
   "metadata": {},
   "source": [
    "### CSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_ROOT = 'path/to/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/WFDBRecords/'\n",
    "OUTPUT_DIR = 'path/to/output/csn/'\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1 # This will be 10% of the original data, meaning VAL_SIZE / (1 - TEST_SIZE) of the training+validation set.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "def parse_header_comments_manual(hea_filepath):\n",
    "    \"\"\"\n",
    "    Manually and safely parses a .hea file to extract only the '#Dx' comment lines.\n",
    "    This bypasses an issue in the wfdb library's internal date parsing logic for this specific dataset.\n",
    "    \n",
    "    Args:\n",
    "        hea_filepath (str): The full path to the .hea file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of SNOMED CT diagnosis codes.\n",
    "    \"\"\"\n",
    "    dx_codes = []\n",
    "    try:\n",
    "        with open(hea_filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip().startswith('#Dx:'):\n",
    "                    codes_str = line.split(':')[1].strip()\n",
    "                    if codes_str:\n",
    "                        dx_codes = [code.strip() for code in codes_str.split(',')]\n",
    "                    break # Stop after finding the Dx line for efficiency\n",
    "    except IOError:\n",
    "        # Return an empty list if the file cannot be opened\n",
    "        return []\n",
    "    return dx_codes\n",
    "\n",
    "def process_ecg_data():\n",
    "    \"\"\"\n",
    "    Main function to find, process, split, and save ECG data.\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Step 1: Discovering all ECG records ---\")\n",
    "    data_path = os.path.abspath(DATA_ROOT)\n",
    "    \n",
    "    # Find all .hea files recursively and store their full paths\n",
    "    all_hea_files = sorted([\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(data_path)\n",
    "        for file in files if file.endswith('.hea')\n",
    "    ])\n",
    "    \n",
    "    if not all_hea_files:\n",
    "        print(f\"Error: No .hea files were found in the directory '{data_path}'.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found a total of {len(all_hea_files)} ECG records.\")\n",
    "\n",
    "    print(\"\\n--- Step 2: Extracting signals and labels (using manual header parsing) ---\")\n",
    "    all_signals = []\n",
    "    all_raw_labels = []\n",
    "    \n",
    "    successful_reads = 0\n",
    "    failed_reads = 0\n",
    "\n",
    "    for hea_filepath in tqdm(all_hea_files, desc=\"Reading records\"):\n",
    "        # Construct the record path base (without extension) from the .hea file path\n",
    "        # Example: '/path/to/JS00001.hea' -> '/path/to/JS00001'\n",
    "        record_path_base = hea_filepath[:-4]\n",
    "        \n",
    "        try:\n",
    "            # 1. Read the signal (.mat file) using the absolute path.\n",
    "            signal, _ = wfdb.rdsamp(record_path_base)\n",
    "            \n",
    "            # 2. Manually parse the .hea file to get trusted labels.\n",
    "            dx_codes = parse_header_comments_manual(hea_filepath)\n",
    "            \n",
    "            # 3. Validate the data.\n",
    "            if signal is None or signal.shape[1] != 12:\n",
    "                failed_reads += 1\n",
    "                continue\n",
    "            \n",
    "            if not dx_codes: # Skip if no diagnosis codes were found\n",
    "                failed_reads += 1\n",
    "                continue\n",
    "\n",
    "            # Transpose signal from (length, channels) to (channels, length)\n",
    "            # which is a common convention for time-series models.\n",
    "            signal_transposed = signal.T\n",
    "            \n",
    "            all_signals.append(signal_transposed)\n",
    "            all_raw_labels.append(dx_codes)\n",
    "            successful_reads += 1\n",
    "            \n",
    "        except Exception:\n",
    "            # Catch any other potential errors during file processing\n",
    "            failed_reads += 1\n",
    "\n",
    "    print(\"\\n--- Reading complete ---\")\n",
    "    print(f\"Successfully processed {successful_reads} records.\")\n",
    "    print(f\"Failed or skipped {failed_reads} records.\")\n",
    "\n",
    "    if not all_signals:\n",
    "        print(\"\\nError: Failed to load any valid ECG data. Halting execution.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Step 3: Multi-hot encoding labels ---\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(all_raw_labels)\n",
    "    num_classes = len(mlb.classes_)\n",
    "    print(f\"Found {num_classes} unique diagnosis codes in the dataset.\")\n",
    "    \n",
    "    mapping_filepath = os.path.join(OUTPUT_DIR, 'snomed_ct_classes.json')\n",
    "    with open(mapping_filepath, 'w') as f:\n",
    "        json.dump(mlb.classes_.tolist(), f, indent=4)\n",
    "    print(f\"Label class mapping saved to: {mapping_filepath}\")\n",
    "    \n",
    "    print(\"\\n--- Step 4: Assembling feature matrix ---\")\n",
    "    try:\n",
    "        X = np.array(all_signals, dtype=np.float32)\n",
    "    except ValueError:\n",
    "        print(\"Warning: ECG records have inconsistent lengths. Truncating to the shortest length.\")\n",
    "        min_len = min(s.shape[1] for s in all_signals)\n",
    "        print(f\"All records will be standardized to the minimum length found: {min_len}\")\n",
    "        \n",
    "        # Pre-allocate numpy array for efficiency\n",
    "        X = np.zeros((len(all_signals), all_signals[0].shape[0], min_len), dtype=np.float32)\n",
    "        for i, s in enumerate(all_signals):\n",
    "            X[i] = s[:, :min_len]\n",
    "\n",
    "    print(f\"Feature matrix (X) shape: {X.shape}\")\n",
    "    print(f\"Label matrix (y) shape: {y.shape}\")\n",
    "\n",
    "    print(\"\\n--- Step 5: Splitting the dataset ---\")\n",
    "    # Adjust validation size to be a proportion of the train+val set\n",
    "    val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)\n",
    "    \n",
    "    # First, split into training+validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    # Then, split the training+validation set into final training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size_adjusted, random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size:   {X_train.shape[0]}\")\n",
    "    print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "    print(f\"Test set size:       {X_test.shape[0]}\")\n",
    "\n",
    "    print(\"\\n--- Step 6: Saving processed files ---\")\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_features.npy'), X_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_labels.npy'), y_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_features.npy'), X_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_labels.npy'), y_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_features.npy'), X_test)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_labels.npy'), y_test)\n",
    "    \n",
    "    print(f\"\\nTask complete! All files have been saved in '{OUTPUT_DIR}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_ecg_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3c65e",
   "metadata": {},
   "source": [
    "### PTB-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c280751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def load_raw_data(df: pd.DataFrame, sampling_rate: int, path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads raw ECG waveform data from record files specified in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing metadata, including filenames.\n",
    "        sampling_rate: The sampling rate of the ECG signals (100 or 500 Hz).\n",
    "        path: The root directory path where the ECG record files are stored.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the ECG signals.\n",
    "    \"\"\"\n",
    "    if sampling_rate == 100:\n",
    "        filenames = df.filename_lr\n",
    "    else:\n",
    "        filenames = df.filename_hr\n",
    "    \n",
    "    # Read all specified records using wfdb.rdsamp\n",
    "    data = [wfdb.rdsamp(os.path.join(path, f)) for f in filenames]\n",
    "    \n",
    "    # Extract only the signal part from the (signal, metadata) tuples\n",
    "    signals = np.array([signal for signal, meta in data])\n",
    "    return signals\n",
    "\n",
    "def aggregate_diagnostic(scp_codes_dict: dict, agg_df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Aggregates detailed SCP codes into their broader diagnostic superclasses.\n",
    "\n",
    "    Args:\n",
    "        scp_codes_dict: A dictionary of SCP codes for a single record.\n",
    "        agg_df: DataFrame containing the mapping from SCP codes to diagnostic classes.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique diagnostic superclasses for the record.\n",
    "    \"\"\"\n",
    "    superclasses = []\n",
    "    for key in scp_codes_dict.keys():\n",
    "        if key in agg_df.index:\n",
    "            superclass = agg_df.loc[key].diagnostic_class\n",
    "            superclasses.append(superclass)\n",
    "            \n",
    "    return list(set(superclasses))\n",
    "\n",
    "def process_and_save_ptbxl(data_path: str, output_path: str, sampling_rate: int = 100):\n",
    "    \"\"\"\n",
    "    Main function to process the PTB-XL dataset and save it into .npy files\n",
    "    for machine learning tasks.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the root directory of the PTB-XL dataset.\n",
    "        output_path: Path to the directory where the processed .npy files will be saved.\n",
    "        sampling_rate: The desired sampling rate (100 or 500).\n",
    "    \"\"\"\n",
    "    print(\"Starting the processing of the PTB-XL dataset...\")\n",
    "\n",
    "    # 1. Load and preprocess metadata\n",
    "    print(\"Step 1/7: Loading metadata...\")\n",
    "    metadata_path = os.path.join(data_path, 'ptbxl_database.csv')\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col='ecg_id')\n",
    "    # Convert string representation of dictionaries to actual dictionaries\n",
    "    metadata_df.scp_codes = metadata_df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    # 2. Load SCP statements and generate diagnostic superclass labels\n",
    "    print(\"Step 2/7: Generating diagnostic superclass labels...\")\n",
    "    scp_statements_path = os.path.join(data_path, 'scp_statements.csv')\n",
    "    agg_df = pd.read_csv(scp_statements_path, index_col=0)\n",
    "    # Filter for diagnostic SCP codes only\n",
    "    agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "    metadata_df['diagnostic_superclass'] = metadata_df.scp_codes.apply(\n",
    "        lambda x: aggregate_diagnostic(x, agg_df)\n",
    "    )\n",
    "\n",
    "    # 3. Load raw ECG signal data\n",
    "    print(f\"Step 3/7: Loading {sampling_rate}Hz ECG signal data...\")\n",
    "    X = load_raw_data(metadata_df, sampling_rate, data_path)\n",
    "    \n",
    "    # 4. Reshape signal data to (n_samples, n_channels, n_length)\n",
    "    print(\"Step 4/7: Reshaping signal data...\")\n",
    "    X = X.transpose(0, 2, 1)\n",
    "\n",
    "    # 5. Perform multi-hot encoding on the labels\n",
    "    print(\"Step 5/7: Performing multi-hot encoding on labels...\")\n",
    "    all_superclasses = sorted(list(agg_df.diagnostic_class.unique()))\n",
    "    mlb = MultiLabelBinarizer(classes=all_superclasses)\n",
    "    y = mlb.fit_transform(metadata_df['diagnostic_superclass'])\n",
    "    print(f\"Label classes: {mlb.classes_}\")\n",
    "\n",
    "    # 6. Split the dataset using the predefined 'strat_fold'\n",
    "    print(\"Step 6/7: Splitting data into train, validation, and test sets...\")\n",
    "    \n",
    "    # The PTB-XL dataset provides a 'strat_fold' column for a 10-fold\n",
    "    # stratified split based on patients. This ensures that all records from a single\n",
    "    # patient belong to the same fold. Using this column for splitting prevents\n",
    "    # data leakage by keeping patients completely separate across sets.\n",
    "    # The officially recommended split is:\n",
    "    # - Folds 1-8:   Training set\n",
    "    # - Fold 9:      Validation set\n",
    "    # - Fold 10:     Test set\n",
    "    \n",
    "    train_indices = metadata_df['strat_fold'].isin(range(1, 9)) # Folds 1 to 8\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    val_indices = metadata_df['strat_fold'] == 9 # Fold 9\n",
    "    X_val = X[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "\n",
    "    test_indices = metadata_df['strat_fold'] == 10 # Fold 10\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    print(\"\\nDataset split complete. Final shapes:\")\n",
    "    print(f\"  Training set features:   {X_train.shape}, labels: {y_train.shape}\")\n",
    "    print(f\"  Validation set features: {X_val.shape}, labels: {y_val.shape}\")\n",
    "    print(f\"  Test set features:       {X_test.shape}, labels: {y_test.shape}\")\n",
    "\n",
    "    # 7. Save the processed data as .npy files\n",
    "    print(f\"\\nStep 7/7: Saving files to directory: {output_path}...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(output_path, 'train_features.npy'), X_train)\n",
    "    np.save(os.path.join(output_path, 'train_labels.npy'), y_train)\n",
    "    \n",
    "    np.save(os.path.join(output_path, 'val_features.npy'), X_val)\n",
    "    np.save(os.path.join(output_path, 'val_labels.npy'), y_val)\n",
    "    \n",
    "    np.save(os.path.join(output_path, 'test_features.npy'), X_test)\n",
    "    np.save(os.path.join(output_path, 'test_labels.npy'), y_test)\n",
    "\n",
    "    print(\"\\nAll files have been saved successfully!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- User Configuration ---\n",
    "    # Please update these paths to match your local directory structure.\n",
    "    DATA_PATH = 'path/to/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'\n",
    "    OUTPUT_PATH = 'path/to/output/ptb-xl/'\n",
    "    SAMPLING_RATE = 100  # The dataset supports 100Hz and 500Hz\n",
    "    \n",
    "    process_and_save_ptbxl(\n",
    "        data_path=DATA_PATH, \n",
    "        output_path=OUTPUT_PATH, \n",
    "        sampling_rate=SAMPLING_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d5e31",
   "metadata": {},
   "source": [
    "### Physionet2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process the PhysioNet 2017 Challenge dataset.\n",
    "    It loads ECG signals and their corresponding labels, pads the signals to a uniform\n",
    "    length, splits the data into training, validation, and test sets, and saves\n",
    "    them as .npy files.\n",
    "    \"\"\"\n",
    "    # --- 1. Define Paths and Constants ---\n",
    "    # !!! IMPORTANT: Please modify these paths to match your local file structure !!!\n",
    "    DATA_DIR = 'path/to/physionet-2017-challenge-data/'\n",
    "    ECG_DIR = os.path.join(DATA_DIR, 'training2017')\n",
    "    REF_FILE = os.path.join(DATA_DIR, 'REFERENCE-v3.csv')\n",
    "    OUTPUT_DIR = 'path/to/output/physionet2017/'\n",
    "    \n",
    "    # The maximum signal duration is 61s, with a sampling rate of 300Hz.\n",
    "    MAX_SIGNAL_LENGTH = 61 * 300  # 18300\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Label mapping for the four classes: Normal, Atrial Fibrillation, Other, Noisy\n",
    "    LABEL_MAP = {'N': 0, 'A': 1, 'O': 2, '~': 3}\n",
    "\n",
    "    print(\"--- Step 1: Configuration Loaded ---\")\n",
    "    print(f\"Data directory:   {ECG_DIR}\")\n",
    "    print(f\"Reference file:   {REF_FILE}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Max signal length: {MAX_SIGNAL_LENGTH}\")\n",
    "    print(\"-\" * 36)\n",
    "\n",
    "    # --- 2. Load Reference Labels ---\n",
    "    try:\n",
    "        df_labels = pd.read_csv(REF_FILE, header=None, names=['record', 'label'])\n",
    "        records = df_labels['record'].tolist()\n",
    "        labels_dict = df_labels.set_index('record')['label'].to_dict()\n",
    "        print(\"--- Step 2: Labels Loaded Successfully ---\")\n",
    "        print(f\"Found {len(records)} records in the reference file.\")\n",
    "        print(\"-\" * 42)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Reference file not found at '{REF_FILE}'. Please check the DATA_DIR path.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Read and Preprocess ECG Signals ---\n",
    "    print(\"--- Step 3: Reading and Preprocessing ECG Signals ---\")\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for record_name in tqdm(records, desc=\"Processing ECG Records\"):\n",
    "        record_path = os.path.join(ECG_DIR, record_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the WFDB record\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            # The signal shape is (length, channels). Transpose to (channels, length).\n",
    "            signal = record.p_signal.T\n",
    "            \n",
    "            # Pad the signal to the maximum length\n",
    "            current_len = signal.shape[1]\n",
    "            pad_len = MAX_SIGNAL_LENGTH - current_len\n",
    "            # Use 'constant' mode to pad with zeros at the end of the signal\n",
    "            padded_signal = np.pad(signal, ((0, 0), (0, pad_len)), 'constant', constant_values=0)\n",
    "            \n",
    "            all_features.append(padded_signal)\n",
    "            \n",
    "            # Get and map the corresponding label\n",
    "            label_char = labels_dict[record_name]\n",
    "            label_int = LABEL_MAP[label_char]\n",
    "            all_labels.append(label_int)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Record file not found for {record_name}, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {record_name}: {e}, skipping.\")\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X = np.array(all_features, dtype=np.float32)\n",
    "    y = np.array(all_labels, dtype=np.int64)\n",
    "\n",
    "    print(\"--- Signal processing complete ---\")\n",
    "    print(f\"Feature matrix shape (X): {X.shape}\")\n",
    "    print(f\"Label vector shape (y):   {y.shape}\")\n",
    "    print(\"-\" * 33)\n",
    "\n",
    "    # --- 4. Split the Dataset (80% train, 10% validation, 10% test) ---\n",
    "    print(\"--- Step 4: Splitting dataset into train, validation, and test sets ---\")\n",
    "    \n",
    "    # First, split off 10% for the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.1, \n",
    "        random_state=RANDOM_STATE, \n",
    "        stratify=y  # Ensure class distribution is preserved\n",
    "    )\n",
    "\n",
    "    # Then, split the remaining 90% to get 10% of the original data for validation\n",
    "    # (0.1 / 0.9 is approximately 11.1% of the remaining data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, \n",
    "        test_size=(0.1/0.9), \n",
    "        random_state=RANDOM_STATE, \n",
    "        stratify=y_train_val # Preserve class distribution in the new split\n",
    "    )\n",
    "\n",
    "    print(\"--- Dataset splitting complete ---\")\n",
    "    print(f\"Training set shape:   Features {X_train.shape}, Labels {y_train.shape}\")\n",
    "    print(f\"Validation set shape: Features {X_val.shape}, Labels {y_val.shape}\")\n",
    "    print(f\"Test set shape:       Features {X_test.shape}, Labels {y_test.shape}\")\n",
    "    print(\"-\" * 33)\n",
    "\n",
    "    # --- 5. Save Files ---\n",
    "    print(f\"--- Step 5: Saving datasets to '{OUTPUT_DIR}' ---\")\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_features.npy'), X_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_labels.npy'), y_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_features.npy'), X_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_labels.npy'), y_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_features.npy'), X_test)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_labels.npy'), y_test)\n",
    "\n",
    "    print(\"--- All files saved successfully. ---\")\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtwbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
